---
title: "Understanding Regression with code"
output: 
 html_notebook:
        toc: true
        number_sections: true
        theme: cerulean
        highlight: tango
---

<hr>
<br>

# Make toy data

1. X: N X p 개의 정규화된 Input Data를 생성한다. 
2. b: p개의 변수의 coefficient가 될 , True 값을 설정한다. 
3. y: 데이터 X에서 b에서 설정한 coefficient가 나올 수 있도록 y값을 생성한다.

즉, y ~ X를 예측했을 때, 각 변수의 coefficient는 b에 가까워야 한다.

```{r}
set.seed(1050)
N <- 300
p <- 6
X <- scale(matrix(rnorm(N*p), ncol=p)) 
b <- c(.5, -.5, .25, -.25,  0, 0)
y <- scale(X %*% b + rnorm(N, sd=.5))
```
```{r}
head(data.frame(y=y, X=X))
```

# Linear Regression
## use lm function in R package
```{r}
lr_pack <- lm(y ~ .-1, data=data.frame(X=X, y=y))
lr_pack$coefficients
```

## use OLS (ordinary least squares)
$\hat \beta  = {({X^T}X)^{ - 1}}{X^T}y$

```{r}
lr_ols <- solve(crossprod(X)) %*% crossprod(X, y)
as.numeric(lr_ols)
```

## use Optimization
### set loss function
$\min {(y - \beta X)^2}$
```{r}
lr_loss <- function(w, X, y) {
    crossprod(y - X%*%w)
}
```

### optimization
`optim` function 사용. <br>
첫 번째 파라미터는 beta의 초기 값. 변수 길이 만큼 셋팅. 아래는 0으로 세팅. <br>
두 번째 파라미터는 위에서 설정한 loss function. <br>
세,네 번째 파라미터는 각각 Input, Output. <br>
다섯 번째 파라미터 method는 최적화 알고리즘. 아래 BFGS는 뉴턴메소드로 알려져 있음. 

```{r}
lr_optim <- optim(rep(0, ncol(X)), lr_loss, X=X, y=y, method='BFGS')
lr_optim$par
```

<br>

# Ridge Regression
## use glmnet function in R glmnet package
`glmnet` 함수의 alpha가 0이면 ridge regression 1이면 lasso regression을 사용한다. <br>
lambda는 여러 lambda 값을 입력해서 모델을 생한하고 최적의 lambda 값을 선택한다.
```{r}
library(glmnet)
glmnet_ridge <- glmnet(X, y, alpha=0, lambda=c(10, 1, .1), intercept=F)
as.numeric(coef(glmnet_ridge, s=.1))[-1]
```

## use Optimization
### set loss function
$\min {(y - \beta X)^2} + \lambda n\sum\limits_i {\beta_i^2}$
```{r}
ridge_loss <- function(w, X, y, lambda = .1) {
  crossprod(y - X%*%w) + lambda*length(y)*crossprod(w)
}
```
### optimization
```{r}
ridge_optim <- optim(rep(0, ncol(X)), ridge_loss, X=X, y=y, lambda=.1, method = 'BFGS')
ridge_optim$par
```

## use OLS (ordinary least squares)
$\hat \beta  = {({X^T}X + \lambda nI)^{ - 1}}{X^T}y$
```{r}
ridge_ols <- solve(crossprod(X) + diag(length(y)*.1, ncol(X))) %*% crossprod(X, y)
as.numeric(ridge_ols)
```

## change coefficient according to the change of lambda
$X^TX$
```{r}
crossprod(X)
```

$X^TX + \lambda nI$
```{r}
print(length(y)*.1)
crossprod(X) + diag(length(y)*.1, ncol(X))
```

change $\lambda$ 0.1 to 1
```{r}
print(length(y)*1)
crossprod(X) + diag(length(y)*1, ncol(X))
```

<br>
**why?** <br>
normalized된 벡터의 내적은 왜 항상 n-1 이 되는가??
```{r}
a <- scale(rnorm(100))
print(crossprod(a))

b <- scale(runif(50))
print(crossprod(b))

```
**anwser** <br>
sample 된 표본 X의 분산은 $Var(X) = \sum\limits_i {{{({x_i} - \bar X)}^2}} /(n - 1)$ 이다. <br>
위 a는 X의 $a = (X - \bar X)/sd(X)$ 로 scale된 값이다. <br>
a의 crossproduct는 ${a^T}a = \sum\limits_i {{{({x_i} - \bar X)}^2}} /sd{(X)^2}$ 이다. <br>
$sd{(X)^2} = Var(X)$ 때문에, ${a^T}a = n-1$이 된다.<br>
<br>

inverse of matrix <br>
$\lambda$ 0.1
```{r}
solve(crossprod(X) + diag(length(y)*.1, ncol(X))) 
```

$\lambda$ 1
```{r}
solve(crossprod(X) + diag(length(y)*1, ncol(X))) 
```

[1,1] 값에서 $\lambda$의 값이 0.1에서 1로 변하면서 `r 3.107341e-03/1.680691e-03`배 줄어들었지만, <br>
[1,2] 값에서는 `r 1.063936e-04/3.516282e-05`배 감소하였다. [1,3]에서는 `r -1.564764e-04/-4.913578e-05` 줄어들었다. <br>
각 weigth 들이 일정하게 증감하지 않는다. 하지만 $\lambda$가 커짐에 따라 0 방향으로 감소한다.

```{r}
crossprod(X, y)
```

```{r}
lamda.1 <- solve(crossprod(X) + diag(length(y)*.1, ncol(X)))  %*% crossprod(X, y)
as.numeric(lamda.1)
```
```{r}
lamda <- solve(crossprod(X) + diag(length(y)*1, ncol(X)))  %*% crossprod(X, y)
as.numeric(lamda)
```
```{r}
as.numeric(lamda.1 / lamda) 
```
동일한 $\lambda$를 증가시킴에 따라, 절대 cofficients 값들이 줄어들지만, 일정 비율로 감소하지 않는다.

### understanding lamda and t
<center><img src="https://jamesmccammondotcom.files.wordpress.com/2014/04/screen-shot-2014-04-19-at-11-19-00-pm.png"></center>

$\hat \beta  = {({X^T}X + \lambda nI)^{ - 1}}{X^T}y$ <br>
위 식에서 $\lambda$가 0이면 일반 linear regression의 베타 해를 구하는 식과 동일해 진다. <br>
그리고 $\lambda$를 키우면 앞의 식의 갚을 키우고 Inverse 함으로 전체 값(베타)이 줄어 든다. <br>
**But** <br>
위 그림에서 제약식인 동그란 원이 커지면 $\beta$ 값이 커지고 원이 작아지면 $\beta$ 값이 0에 가까워 져야 한다. <br>
위 식에서 이해한 상식과 반대로 작동한다. <br>
**what is the problem?** <br>
위 식에서 $\lambda$가 그림에서 원의 크고 작음을 나타내지 않는다. <br>
**understanding lamda & t** <br>
ridge의 최초 minimize 식을 생각하면 다음과 같다. <br>
$\min {(y - \beta X)^2}/n$ subject to $\sum\limits_i {{\beta _i}^2}  < t$ <br>
위 식은 다시 lagrangian에 의해 다음과 동일하게 된다. <br>
$\min {(y - \beta X)^2}/n - \lambda (t - \sum {{\beta ^2}} )$ <br>
=> $\min {(y - \beta X)^2}/n + \lambda (\sum {{\beta ^2} - t} )$ <br>
derivative $\beta  = {({X^T}X + \lambda nI)^{ - 1}}{X^T}y$ <br>
<br>
$\beta$ 값을 생각했을 때, $\lambda$ 값이 0에 가까워지면, 일반 linear regression 식과 동일해진다. <br>
반대로 $\lambda$ 값을 키우면 식의 오른쪽 텀의 값이 커지고 역함수가 되기 때문에 $\beta$ 값은 줄어들게 된다. <br>
이번엔 제약식에서 t 값을 높인다면 $\beta$ 값이 커지고, t 값이 0에 가까워 지면 beta 값이 작아진다. <br>
즉, $\lambda$ 값을 키우는 것이 t 값을 줄이는 것과 같은 효과를 보이고 $\lambda$ 줄이는 것이 t 값을 높이는 것과 같아 진다. <br>

# Lasso Regression
## use glmnet function in R glmnet package
`glmnet` 함수의 alpha가 0이면 ridge regression 1이면 lasso regression을 사용한다. <br>
$\lambda$는 여러 $\lambda$ 값을 입력해서 모델을 생성하고 최적의 $\lambda$ 값을 선택한다.
```{r}
library(glmnet)
glmnet_lasso <- glmnet(X, y, alpha=1, lambda=c(10, 1, .1), thresh=1e-12, intercept=F)
as.numeric(coef(glmnet_lasso, s=.1))[-1]
```

## use Optimization
### set loss function
$\min {(y - \beta X)^2} + \lambda n\sum\limits_i {abs({\beta _i})}$
```{r}
lasso_loss <- function(w, X, y, lambda = .1) {
  crossprod(y - X%*%w) + lambda*length(y)*sum(abs(w))
}
```
### optimization
```{r}
lasso_optim <- optim(rep(0, ncol(X)), lasso_loss, X=X, y=y, lambda=.1, method = 'BFGS')
lasso_optim$par
```

<br>

lasso의 경우 정규화(제약)텀이 미분되지 않기 때문에 OLS 방법으로 해를 구할 수 없다. <br>
그리고 일반적으로 나타내는 lasso_loss 식을 통해 일반적으로 사용되는 최적화 식으로 풀었을 때, <br>
lasso package에서 작동하는 방식으로 동작하지는 않는다. <br>
lasso의 $\beta$ 해를 구하는 방법이 일반적인 방법과 다른 방법으로 풀어야 한다. ([참조](http://statweb.stanford.edu/~tibs/sta305files/Rudyregularization.pdf)) <br>

<br>

# Ordianl Regression (cumulative link model)
(참조: [Cumulative Link Models for Ordinal Regression with the R Package ordinal](https://cran.r-project.org/web/packages/ordinal/vignettes/clm_article.pdf))

## Make toy data
```{r}
set.seed(1050)
N <- 1000                                       # 샘플 수
x <- cbind(x1 = rnorm(N), x2 = rnorm(N))        # 두개의 랜덤노말 변수 생성
beta <- c(1,-1)                                 # coefficients
y_star <- rnorm(N, mean=x %*% beta)             # true y (1*x1 + -1*x2)
y_1 <- y_star > -1.5                            # -1.5 theta 1
y_2 <- y_star > .75                             # .75 theta 2
y_3 <- y_star > 1.75                            # 1.75 theta 3
y <- y_1 + y_2 + y_3 + 1                        # target
df <- data.frame(x, y=factor(y))
```
labeling

1. y <= -1.5
2. -1.5 < y <= 0.75
3. 0.75 < y <= 1.75
4. y > 1.75 

distribution of y
```{r}
table(y)
```

## use clm function in R ordinal package
```{r}
library(ordinal)
clm_pack_probit = clm(y ~ x1 + x2, data=df, link='probit')
clm_pack_logit  = clm(y ~ x1 + x2, data=df, link='logit')
```
```{r}
print("probit link:")
print(coef(clm_pack_probit))
cat('\n')
print("logit link:")
print(coef(clm_pack_logit))
```

## use Optimization
```{r}
probit=T
pfun = ifelse(probit, pnorm, plogis)

K = length(unique(y))
ncuts = K-1

par = c(-1,1,2,0,0)
cuts = par[(1:ncuts)]
beta = par[-(1:ncuts)]
```

1. probit & pfun
    - link function을 선택한다. 
    - probit이 True라면 정규분포를 사용하고 False라면 logistic 분포를 사용한다.
2. K & ncuts
    - K는 ordinal label의 길이고 ncuts은 ordinal parameter 수이다.
3. par: 초기 파라미터를 설정한다.
    - [cuts] + [beta]
    - cuts: 위 예에서 라벨의 길이는 4(1,2,3,4) 이기 때문에 3개(theta1, 2, 3)의 파라미터를 설정한다.
    - beta: 변수(피처)의 개수는 2개이기 때문에 2개의 파라미터를 설정한다.

y_h: linear calculate ( beta1\*x1 + beta2\*x2 )
```{r}
y_h = x %*% beta
print(head(y_h))
```

ll: log-likelihood
```{r}
ll = rep(0, length(y))
print(head(ll))
```

γij = F(ηij), ηij = θj − x⊤iβ <br>
γij = P(Yi ≤ j) = πi1 + . . . + πij with sum(πij) = 1 <br>
<br>

j=1 일때, ηi1 = -1 - x^t %*% beta <br>
j=2 일때, ηi2 = 1 - x^t %*% beta <br>
j=3 일때, ηi3 = 2 - x^t %*% beta <br>

<br>
j=1, γi1 = P(Yi ≤ 1) = F(ηi1) <br>
j=2, γi2 = P(Yi ≤ 2) = F(ηi2) - F(ηi1) <br>
j=3, γi3 = P(Yi ≤ 3) = F(ηi3) - F(ηi2) <br>
j=4, γi4 = P(Yi ≤ 4) = 1 - F(ηi3) <br>

```{r}
g1 <- pfun((cuts[1] - lp))
g2 <- pfun(cuts[2] - lp) - pfun(cuts[2-1] - lp)
g3 <- pfun(cuts[3] - lp) - pfun(cuts[3-1] - lp)
g4 <- 1 - pfun(cuts[4-1] - lp)
```

```{r}
print(a1[1:5])
print(a2[1:5])
print(a3[1:5])
print(a4[1:5])
print(a1[1:5] + a2[1:5] + a3[1:5] + a4[1:5])
```

**likelihood: multiply( yij_h ^ yij )**
<br>
**log-likelihood: sum_i( sum_j( yij * log(yij_h) ))** <br>
**=> sum_i( yi * log(yi_h) ) (j는 같은 label에서 1이고 나머지는 0이기 때문에)** <br>

code로 다시쓰면 아래와 같다.
```{r}
for(k in 1:K) {
    if (k==1) {
      ll[y==k] = pfun((cuts[k] - lp[y==k]), log = TRUE)
    }
    else if (k < K) {
      ll[y==k] = log(pfun(cuts[k] - lp[y==k]) - pfun(cuts[k-1] - lp[y==k]))
    }
    else {
      ll[y==k] = log(1 - pfun(cuts[k-1] - lp[y==k])) 
    }
}

loss <- -sum(ll)
```

loss function으로 다음과 같이 묶을 수 있다.
```{r}
clm_loss = function(par, X, y, probit=T) {
  K = length(unique(y))
  ncuts = K-1
  cuts = par[(1:ncuts)]
  beta = par[-(1:ncuts)]
  lp = X %*% beta
  ll = rep(0, length(y))
  pfun = ifelse(probit, pnorm, plogis)
  
  for(k in 1:K){
    if (k==1) {
      ll[y==k] = pfun((cuts[k] - lp[y==k]), log = TRUE)
    }
    else if (k < K) {
      ll[y==k] = log(pfun(cuts[k] - lp[y==k]) - pfun(cuts[k-1] - lp[y==k]))
    }
    else {
      ll[y==k] = log(1 - pfun(cuts[k-1] - lp[y==k])) 
    }
  }
  return(-sum(ll))
}
```
<br>

optimization
```{r}
par = c(-1,1,2,0,0)
clm_optim_probit = optim(par, clm_loss, y = y, X = x, probit=T,control=list(reltol=1e-10))
clm_optim_logit  = optim(par, clm_loss, y = y, X = x, probit=F,control=list(reltol=1e-10))
```
```{r}
print("probit link:")
print(clm_optim_probit$par)
cat('\n')
print("logit link:")
print(clm_optim_logit$par)
```

이전 clm 함수의 결과와 비교해 보자
```{r}
print("probit link:")
print(coef(clm_pack_probit))
cat('\n')
print("logit link:")
print(coef(clm_pack_logit))
```

## Interpretation
```{r}
pred <- predict(clm_pack_logit, newdata = subset(df, select = -y))$fit
```
```{r}
head(cbind(x, y,pred))
```

```{r}
beta_h <- coef(clm_pack_logit)[4:5]

y_h <- x %*% beta_h
head(y_h)
```

```{r}
theta <- coef(clm_pack_logit)[1:3]

p0_1 <- 1 / (1 + exp(-(theta[1] - y_h)))
p0_2 <- 1 / (1 + exp(-(theta[2] - y_h)))
p0_3 <- 1 / (1 + exp(-(theta[3] - y_h)))

p1 <- p0_1
p2 <- p0_2 - p0_1
p3 <- p0_3 - p0_2
p4 <- 1 - p0_3
```

```{r}
head(cbind(x, y, p1, p2, p3, p4))
```

